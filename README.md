# üé≠ Tweet Emotion Classifier via Transformer

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange)
![License](https://img.shields.io/badge/License-MIT-green)

Ce projet impl√©mente un mod√®le d'IA bas√© sur l'architecture **Transformer** (construit "from scratch") pour classer les √©motions dans des tweets. Le mod√®le est capable de distinguer 6 √©motions principales : Joie, Tristesse, Col√®re, Peur, Amour et Surprise.

## üìÑ Description du Projet

L'objectif est de d√©montrer l'efficacit√© de l'architecture Transformer (Self-Attention) pour le traitement du langage naturel (NLP) sur des textes courts et informels (tweets). Contrairement √† l'utilisation de mod√®les pr√©-entra√Æn√©s (comme BERT), ce projet construit et entra√Æne un encodeur Transformer l√©ger enti√®rement personnalis√© avec TensorFlow/Keras.

### Caract√©ristiques principales :
* **Nettoyage de texte** : Suppression des URLs, mentions (@user) et caract√®res sp√©ciaux via Regex.
* **Architecture** : Couche d'Embedding personnalis√©e (Token + Position) et Bloc Transformer (Multi-Head Attention).
* **Entra√Ænement** : Optimisation avec Adam, gestion du sur-apprentissage via Dropout et EarlyStopping.
* **Sauvegarde** : Checkpoint automatique du meilleur mod√®le sur Google Drive.

## üìä Dataset

Le mod√®le est entra√Æn√© sur le dataset **[dair-ai/emotion](https://huggingface.co/datasets/dair-ai/emotion)** disponible sur Hugging Face.

* **Taille** : ~20 000 tweets (Train/Val/Test).
* **Classes** :
    * 0: Sadness (Tristesse)
    * 1: Joy (Joie)
    * 2: Love (Amour)
    * 3: Anger (Col√®re)
    * 4: Fear (Peur)
    * 5: Surprise

## üß† Architecture du Mod√®le

Le mod√®le n'utilise pas de RNN (LSTM/GRU) mais repose enti√®rement sur l'attention :

1.  **Input Layer** : S√©quences de longueur fixe (100 tokens).
2.  **Token & Position Embedding** : Combine le sens du mot et sa position dans la phrase.
3.  **Transformer Block** :
    * Multi-Head Attention (2 t√™tes).
    * R√©seau Feed-Forward (Dense).
    * Normalisation (LayerNorm) et connexions r√©siduelles.
4.  **Global Average Pooling** : R√©duction de la dimensionnalit√©.
5.  **Output Layer** : Dense + Softmax (6 neurones).

## üìà Performances et R√©sultats

* **Accuracy (Test)** : ~89%.
* **Observations** : Le mod√®le distingue tr√®s bien les √©motions √† forte valence (Joie vs Col√®re) mais peut pr√©senter des confusions sur des nuances s√©mantiques proches (ex: Tristesse vs Peur).


`![Confusion Matrix](./Matrice_confusion.png)`

## üõ†Ô∏è Installation et Utilisation

### Pr√©requis
* Python 3.x
* TensorFlow / Keras
* Datasets (Hugging Face)
* Pandas, NumPy, Scikit-learn, Matplotlib

### Installation
```bash
pip install tensorflow datasets pandas numpy scikit-learn matplotlib seaborn
